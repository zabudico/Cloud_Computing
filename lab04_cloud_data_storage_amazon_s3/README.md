# Лабораторная работа №4. Облачное хранилище данных. Amazon S3

## Цель работы

Целью работы является познакомиться с сервисом Amazon S3 (Simple Storage Service) и отработать основные операции:

* создание публичного и приватного бакетов;
* загрузку и организацию объектов;
* работу с S3 через AWS CLI (копирование, перемещение, синхронизация);
* настройку версионирования и шифрования;
* использование S3 Static Website Hosting;
* применение Lifecycle-правил для архивирования старых данных.


### После выполнения работы:

* понимаю концепцию объектного хранилища и его отличия от блочного и файлового;
* умею создавать и настраивать бакеты в Amazon S3;
* владею навыками загрузки и управления объектами через консоль и CLI;
* знаю, как настроить статический веб-хостинг на базе S3;
* понимаю принципы версионирования и жизненного цикла объектов в S3.


### Постановка задачи

Amazon S3 — это объектное хранилище AWS, предназначенное для хранения файлов любого типа: изображений, резервных копий, документов, логов и т.д. Каждый объект хранится внутри бакета (bucket) и имеет уникальный ключ (key). "Папки" в консоли — это лишь префиксы ключей, а не настоящие директории.

#### Основные этапы работы

В этой лабораторной создал два бакета:

* Публичный бакет - для хранения аватаров пользователей и статического контента;

* Приватный бакет - для логов и служебных файлов (с Lifecycle-политикой).


#### Шаг 1. Подготовка

Регион: eu-central-1 (Frankfurt).

Формат имён бакетов:

* Публичный: `cc-lab4-pub-k21-zabudico-alexandr-21-21`

* Приватный: `cc-lab4-priv-k21-zabudico-alexandr-21-21`

( kXX - k21 номер по журналу )

Локально (на своем компьютере) создал структуру каталогов и файлов, как показано ниже:

```
s3-lab/
 ├── public/
 │   ├── avatars/
 │   │   ├── user1.jpg
 │   │   └── user2.jpg
 │   └── content/logo.png
 ├── private/
 │   └── logs/
 │       └── activity.csv
 └── README.md
 ```

Данные файлы в дальнейшем будут загружены в соответствующие бакеты.

#### Шаг 2. Создание бакетов

На данном шаге могу выбрать один из способов создания бакетов:

* использовать ACL (упрощённый, наглядный)

* использовать Object Ownership Enforced (современный, безопасный).

Ссылка на данный вариант - [ссылка](https://github.com/MSU-Courses/cloud-computing/blob/main/_lab/04_Cloud_Storage/advanced.md)

`Чем отличаются два способа управления доступом к бакетам в S3`

В данной работе описан первый способ (с использованием ACL).

Перешёл в консоль управления AWS S3 и создал два бакета с указанными именами.

Публичный бакет:

* Имя: cc-lab4-pub-k21-zabudico-alexandr-21-21
* Region: eu-central-1
* Object Ownership: ACLs enabled (Can be configured using ACLs)
* Block all public access: снять галочку (разрешить публичность)

Подтвердил предупреждение

Нажал `Create bucket`

![create_bucket](/lab04_cloud_data_storage_amazon_s3/img/aws_bucket_1.png)

`Что означает опция “Block all public access” и зачем нужна данная настройка?`

Аналогично создал Приватный бакет с именем cc-lab4-priv-k21-zabudico-alexandr-21-21, но оставил все настройки по умолчанию (Block all public access — включен).

Режим ACLs enabled позволяет управлять доступом к каждому объекту отдельно (например, сделать одну картинку публичной, а другую — приватной). Это удобно для лабораторных экспериментов и визуально понятно.

#### Шаг 3. Загрузка объектов через AWS Management Console

Перешёл в бакет 
cc-lab4-pub-k21-zabudico-alexandr-21-21.

Перешёл в директорию avatars/ и Нажал Upload.

Загрузил файл user1.jpg из локальной папки s3-lab/public/avatars/.

После загрузки в пункте Permissions выбрал Grant public-read access.

Завершил загрузку нажав Upload.

![step3](/lab04_cloud_data_storage_amazon_s3/img/aws_bucket_upload_public_bucket_3.png)


`Чем отличается ключ (object key) от имени файла?`

#### Шаг 4. Загрузка объектов через AWS CLI

Установил и настроил AWS CLI, так как это не было сделано ранее.

Выполнил команду для загрузку файла user2.jpg в публичный бакет:

```bash
aws s3 cp s3-lab/public/avatars/user2.jpg s3://
cc-lab4-pub-k21-zabudico-alexandr-21-21/avatars/user2.jpg --acl public-read
```

![step4](/lab04_cloud_data_storage_amazon_s3/img/aws_bucket_upload_public_bucket_3_2.png)

Загрузил файл logo.png в публичный бакет, в директорию content/, также сделав его публичным.

![step4_1](/lab04_cloud_data_storage_amazon_s3/img/aws_bucket_upload_public_bucket_3_2_1.png)


Загрузил файл activity.csv в приватный бакет, не делая его публичным.

![step4_2](/lab04_cloud_data_storage_amazon_s3/img/aws_bucket_upload_public_bucket_3_2_2.png)

`В чём разница между командами aws s3 cp, mv и sync и для чего используется параметр флаг --acl public-read?`

#### Шаг 5. Проверка доступа к объектам

Открыл в браузере URL загруженного публичного объекта:

```
https://
cc-lab4-pub-k21-zabudico-alexandr-21-21.s3.eu-central-1.amazonaws.com/avatars/user1.jpg
```

![step5](/lab04_cloud_data_storage_amazon_s3/img/aws_bucket_upload_public_bucket_5.png)

Увидел изображение.

Попробывал открыть URL загруженного приватного объекта, он был недоступен.

![step5](/lab04_cloud_data_storage_amazon_s3/img/aws_bucket_upload_priv_bucket_5_2.png)

#### Шаг 6. Версионирование объектов

Включил версионирование для обоих бакетов через вкладку 

Properties → Bucket Versioning → Enable.

Изменил файл logo.png и Загрузил его заново, чтобы увидел создание новой версии.

Просмотрел вкладку Versions, там отображаются все версии объекта.

`Что произойдёт, если выключить версионирование после его включения?`

![step6](/lab04_cloud_data_storage_amazon_s3/img/aws_bucket_upload_public_bucket_6_versions.png)

![step6](/lab04_cloud_data_storage_amazon_s3/img/aws_bucket_upload_public_bucket_6_versions_1.png)

#### Шаг 7. Создание Lifecycle-правил для приватного бакета

В приватном бакете зашёл в Management → Lifecycle rules → Create rule.

* Имя: logs-archive
* Префикс: logs/
* Actions:
* Transition → Standard-IA через 30 дней
* Transition → Glacier Deep Archive через 365 дней
* Expiration → удалить через 1825 дней (5 лет)

Сохранил правило: `Create rule`.

`Что такое Storage Class в Amazon S3 и зачем они нужны?`

Так автоматизирую хранение логов: старые файлы будут автоматически архивироваться.

![step7](/lab04_cloud_data_storage_amazon_s3/img/aws_bucket_Lifecycle_7.png)

#### Шаг 8. Создание статического веб-сайта на базе S3

Создал бакет cc-lab4-web-k21 для хостинга статического сайта:

* Имя: cc-lab4-web-kXX
* Region: eu-central-1
* Object Ownership: ACLs enabled (Can be configured using ACLs)
* Block all public access: снять галочку (разрешить публичность)

Нажал `Create bucket`

После создания настроил хостинг:

Перешёл в бакет → вкладка Properties → Static website hosting → Edit -> Enable.

Выбрал следующие параметры:

* Hosting type: Host a static website.
* Index document: index.html.

Проанализировал веб-сайт, прикрепленный к данной лабораторной работе: веб-сайт

Скопировал файлы веб-сайта в бакет cc-lab4-web-k21. Не забыл сделать файлы публичными!

![step8](/lab04_cloud_data_storage_amazon_s3/img/aws_bucket_website_8.png)

Открыл URL статического сайта, указанный в настройках S3.

С помощью S3 можно быстро и просто развернуть статический сайт без необходимости использования серверов. Например, если есть React-приложение, его собранные файлы можно загрузить в такой бакет для хостинга.

Несмотря на удобство, для серьёзных проектов рекомендуется использовать специализированные сервисы хостинга (например, AWS Amplify), которые предоставляют дополнительные возможности.


#### Шаг 9. Дополнительное задание. Загрузка файлов через AWS SDK

Дан небольшой скрипт на PHP, который позволяет загружать файлы в S3 через веб-форму.

Форма для загрузки файла (upload.php)

```php
<!-- index.php -->
<form action="upload.php" method="post" enctype="multipart/form-data">
  Select file to upload:
  <input type="file" name="fileToUpload" id="fileToUpload" />
  <input type="submit" value="Upload File" name="submit" />
</form>
```

Обработчик загрузки файла (upload.php)
```php
<?php

// upload.php

require 'vendor/autoload.php';

use Aws\S3\S3Client;

define('BUCKET_NAME', 'bucket-name');

$s3 = new S3Client([
    'region'  => 'us-central-1',
]);

// Получение файла из формы
$file = $_FILES['fileToUpload']['tmp_name'];
$filename = $_FILES['fileToUpload']['name'];

// Поместить в "папку" avatars (S3 использует префиксы в ключах)
$destinationKey = 'avatars/' . basename($filename);

// Загрузка файла в S3
try {
    $result = $s3->putObject([
        'Bucket'     => BUCKET_NAME,
        'Key'        => $destinationKey,
        'SourceFile' => $file,
        'ACL'        => 'public-read', // Сделать файл публичным
    ]);

    echo "File uploaded successfully. URL: " . $result['ObjectURL'];
} catch (Aws\Exception\AwsException $e) {
    echo "Error uploading file: " . $e->getMessage();
}
```

Добавил следующую функциональность:

При загрузке аватара, поменял имя файла на уникальное (например, добавил временную метку или UUID), чтобы избежать конфликтов имён.

Добавил базу данных (рекомендуется SQLite) для хранения информации о загруженных файлах (оригинальное имя, новое имя, URL, дата загрузки).

Ввёл список всех загруженных файлов на отдельной странице.

### Контрольные вопросы

`Чем отличаются два способа управления доступом к бакетам в S3?`

Два способа управления доступом: ACL (старый) — для отдельных объектов, позволяет давать права каждому файлу. Object Ownership (новый) — через политики бакета, делает владельцем бакета все объекты, проще и безопаснее.

`Что означает опция “Block all public access” и зачем нужна данная настройка?`

Опция “Block all public access” блокирует любой публичный доступ к бакету, даже если права позволяют. Нужна для защиты от случайных утечек данных.

`Чем отличается ключ (object key) от имени файла?`

Ключ (object key) — это полный уникальный ID объекта в бакете, включая "папки" как префиксы. Имя файла — просто последняя часть ключа, без пути.

`В чём разница между командами aws s3 cp, mv и sync и для чего используется параметр флаг --acl public-read?`

cp — копирует файл. mv — перемещает (копирует и удаляет оригинал). sync — синхронизирует папки, копирует только изменения. Флаг --acl public-read делает объект доступным для чтения всем.

`Что произойдёт, если выключить версионирование после его включения?`

Если выключить версионирование, старые версии файлов останутся, но новые загрузки не будут создавать версии.

`Что такое Storage Class в Amazon S3 и зачем они нужны?`

Storage Class — это типы хранения в S3 (например, Standard для частого доступа, Glacier для архива). Нужны, чтобы экономить деньги: дешёвые для редких файлов, быстрые для нужных.

## Список использованных источников

* Официальная документация AWS S3: https://docs.aws.amazon.com/s3/index.html

* GitHub MSU-Courses: https://github.com/MSU-Courses/cloud-computing/blob/main/_lab/04_Cloud_Storage/advanced.md

* Документация AWS CLI для S3: https://docs.aws.amazon.com/cli/latest/reference/s3/

* Документация AWS SDK для PHP: https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/

## Вывод

В лабораторной работе №4 по Amazon S3 я узнал, как работает облачное хранилище для файлов. Я создал публичные и приватные бакеты, загрузил файлы через консоль и CLI, настроил доступ и публичность. Освоил команды cp, mv и sync, а также флаг для публичного доступа. Включил версионирование для сохранения старых версий файлов и lifecycle-правила для автоматического архивирования. Настроил статический сайт на S3 — просто загрузил файлы и сделал их публичными. В допзадании доработал PHP-скрипт: добавил уникальные имена файлов и базу SQLite для хранения данных о загрузках. Понял, что S3 отличается от обычных дисков: здесь файлы — объекты с ключами, а классы хранения помогают экономить деньги. Теперь могу использовать S3 в реальных проектах.

## Дополнительные важные аспекты

* Безопасность: Для публичных бакетов используй политики вместо ACL в реальных проектах, чтобы избежать утечек. Следи за расходами в AWS.
* Масштаб: S3 держит огромные объёмы данных с высокой надёжностью. Для больших файлов — используй multipart-загрузку.
* Интеграции: S3 работает с другими сервисами AWS, как CloudFront для быстрой доставки или Lambda для автоматизации.
* Ограничения: Бакеты — уникальные по именам глобально, файл до 5 ТБ. Версионирование не удаляет старые версии само.
* Альтернативы: Для динамики — не S3, а EC2 или Amplify. В PHP добавь проверки файлов для безопасности.